{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b07cf2f-b81d-43ef-907a-c49fed2d95ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T20:45:44.515912Z",
     "start_time": "2024-11-21T20:45:24.388939Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torchvision import transforms\n",
    "from progressBar import printProgressBar\n",
    "\n",
    "import medicalDataLoader\n",
    "import argparse\n",
    "from utils import *\n",
    "\n",
    "from UNet_Base import *\n",
    "import random\n",
    "import torch\n",
    "import pdb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2fd9da54de0657",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T20:58:41.235356Z",
     "start_time": "2024-11-21T20:58:41.231783Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "########### HYPERPARAMETERS ###########\n",
    "NUM_EPOCHS = 300\n",
    "BATCH_SIZE = 16\n",
    "BATCH_SIZE_VAL = 8\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "### SELF LEARNING ###\n",
    "SELF_LEARNING_EPOCHS = 50\n",
    "CONFIDENCE_THRESHOLD = 0.97\n",
    "SIMILARITY_THRESHOLD = 0.85\n",
    "GAMMA_LOSS = 0.3\n",
    "\n",
    "### LOSS FUNCTIONS ###\n",
    "# Tversky Focal Loss\n",
    "ALPHA = 0.3\n",
    "BETA = 0.7\n",
    "GAMMA = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220c7dcc-8438-454d-97b1-8e989a7b8f06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T20:58:42.306523Z",
     "start_time": "2024-11-21T20:58:42.268771Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def runTraining(\n",
    "        debug=False, model_name='Test', loss = 'TF',\n",
    "        num_epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, batch_size_val=BATCH_SIZE_VAL, lr=LEARNING_RATE,\n",
    "        self_learning_epochs=SELF_LEARNING_EPOCHS, confidence_threshold=CONFIDENCE_THRESHOLD, similarity_threshold=SIMILARITY_THRESHOLD, gamma_loss=GAMMA_LOSS,\n",
    "        alpha=ALPHA, beta=BETA, gamma=GAMMA\n",
    "    ):\n",
    "    \n",
    "    root_dir = './Data/'\n",
    "\n",
    "\n",
    "    ## DATA TRANSFORMS\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    mask_transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "\n",
    "    ## CREATE THE DATASET\n",
    "    # Training dataset\n",
    "    original_train_set = medicalDataLoader.MedicalImageDataset('train',\n",
    "                                                      root_dir,\n",
    "                                                      transform=transform,\n",
    "                                                      mask_transform=mask_transform,\n",
    "                                                      augment=True,\n",
    "                                                      equalize=True)\n",
    "    train_set_full = original_train_set\n",
    "    # Create a DataLoader for the training set\n",
    "    train_loader_full = DataLoader(train_set_full,\n",
    "                              batch_size=batch_size,\n",
    "                              worker_init_fn=np.random.seed(0),\n",
    "                              num_workers=0,\n",
    "                              shuffle=True)\n",
    "    # Unlabelled dataset\n",
    "    unlabeled_set = medicalDataLoader.MedicalImageDataset('unlabeled',\n",
    "                                                          root_dir,\n",
    "                                                          transform=transform,\n",
    "                                                          mask_transform=mask_transform,\n",
    "                                                          equalize=True)\n",
    "    # Create a DataLoader for the unlabeled set\n",
    "    unlabeled_loader = DataLoader(unlabeled_set,\n",
    "                                  batch_size=batch_size,\n",
    "                                  worker_init_fn=np.random.seed(0),\n",
    "                                  num_workers=0,\n",
    "                                  shuffle=False)\n",
    "    # Validation dataset\n",
    "    val_set = medicalDataLoader.MedicalImageDataset('val',\n",
    "                                                    root_dir,\n",
    "                                                    transform=transform,\n",
    "                                                    mask_transform=mask_transform,\n",
    "                                                    equalize=True)\n",
    "    # Create a DataLoader for the validation set\n",
    "    val_loader = DataLoader(val_set,\n",
    "                            batch_size=batch_size_val,\n",
    "                            worker_init_fn=np.random.seed(0),\n",
    "                            num_workers=0,\n",
    "                            shuffle=False)\n",
    "\n",
    "\n",
    "    ## INITIALIZING THE MODEL\n",
    "    num_classes = 4 # NUMBER OF CLASSES\n",
    "    print(\"~~~~~~~~~~~ Creating the UNet model ~~~~~~~~~~~\")\n",
    "    print(\" Model Name: {}\".format(model_name))\n",
    "    net = UNet(num_classes)\n",
    "    print(\"Total params: {0:,}\".format(sum(p.numel() for p in net.parameters() if p.requires_grad))) if debug else None\n",
    "    \n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    ## OUTPUT COMPONENTS\n",
    "    # Compute class weights for the cross-entropy loss\n",
    "    initial_pixel_counts = compute_class_pixel_counts(train_set_full, num_classes)\n",
    "    class_weights = compute_class_weights(initial_pixel_counts)\n",
    "\n",
    "    softMax = torch.nn.Softmax(dim=1)\n",
    "    TF_loss = TverskyFocalLoss(alpha=alpha, beta=beta, gamma=gamma)\n",
    "    CE_loss = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "    DSC_loss = DiceLoss()\n",
    "    if torch.cuda.is_available(): # Move to GPU if available\n",
    "        net.cuda()\n",
    "        softMax.cuda()\n",
    "        TF_loss.cuda()\n",
    "        CE_loss.cuda()\n",
    "        DSC_loss.cuda()\n",
    "    \n",
    "    # Define the loss function\n",
    "    def loss(predictions, targets):\n",
    "        if loss == 'TF':\n",
    "            return TF_loss(predictions, targets)\n",
    "        elif loss == 'CE':\n",
    "            return CE_loss(predictions, targets)\n",
    "        elif loss == 'CE+TF':\n",
    "            return CE_loss(predictions, targets) + TF_loss(predictions, targets)\n",
    "        elif loss == 'DSC':\n",
    "            return DSC_loss(predictions, targets)\n",
    "        else:\n",
    "            return TF_loss(predictions, targets)\n",
    "\n",
    "\n",
    "    ## PSEUDO-LABELED DATA\n",
    "    pseudo_dataset = None\n",
    "    pseudo_loader = None\n",
    "\n",
    "\n",
    "    ## LOSS AND METRICS\n",
    "    # Loss\n",
    "    directory = 'Results/Statistics/' + model_name\n",
    "    if os.path.exists(directory)==False:\n",
    "        os.makedirs(directory)\n",
    "    lossTotalTraining = []\n",
    "    val_losses = []  # List to store validation losses\n",
    "    Best_loss_val = 1000\n",
    "    BestEpoch = 0\n",
    "    # Metrics\n",
    "    metrics_directory = os.path.join(directory, 'Metrics')\n",
    "    if os.path.exists(metrics_directory)==False:\n",
    "        os.makedirs(metrics_directory)\n",
    "    else: # Clear the directory if it exists\n",
    "        for file in os.listdir(metrics_directory):\n",
    "            os.remove(os.path.join(metrics_directory, file))\n",
    "\n",
    "\n",
    "\n",
    "    ## MAIN TRAINING LOOP\n",
    "    print(\"~~~~~~~~~~~~ Starting the training ~~~~~~~~~~~~\")\n",
    "\n",
    "    ## FOR EACH EPOCH\n",
    "    for i in range(num_epochs):\n",
    "        net.train()\n",
    "        lossEpoch = []\n",
    "        val_loss = []\n",
    "\n",
    "        # Keep track of the number of images used for training\n",
    "        num_images_trained = 0\n",
    "        \n",
    "        ########## TRAINING PHASE ##########\n",
    "        # Create iterators for the data loaders\n",
    "        train_loader_iter = iter(train_loader_full)\n",
    "        pseudo_loader_iter = iter(pseudo_loader) if pseudo_dataset is not None else None\n",
    "\n",
    "        num_batches = max(len(train_loader_full), len(pseudo_loader)) if pseudo_dataset is not None else len(train_loader_full)\n",
    "\n",
    "        ## BATCH TRAINING LOOP\n",
    "        for _ in range(num_batches):\n",
    "\n",
    "            # Set to zero all the gradients\n",
    "            net.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            # Get next batch from training data\n",
    "            try:\n",
    "                data_train = next(train_loader_iter)\n",
    "                if len(data_train[0]) == 0:\n",
    "                    continue  # Skip empty batches\n",
    "                \n",
    "                # Process training data\n",
    "                images_train, labels_train, img_names_train = data_train\n",
    "                images_train = to_var(images_train)\n",
    "                labels_train = to_var(labels_train)\n",
    "                net_predictions_train = net(images_train)\n",
    "                segmentation_classes_train = getTargetSegmentation(labels_train)\n",
    "                loss_train = loss(net_predictions_train, segmentation_classes_train)\n",
    "            except StopIteration:\n",
    "                loss_train = 0\n",
    "            \n",
    "            # Get next batch from pseudo-labeled data\n",
    "            if pseudo_loader_iter is not None:\n",
    "                try:\n",
    "                    data_pseudo = next(pseudo_loader_iter)\n",
    "                    if len(data_pseudo[0]) == 0:\n",
    "                        continue  # Skip empty batches\n",
    "\n",
    "                    # Process pseudo-labeled data\n",
    "                    images_pseudo, labels_pseudo, img_names_pseudo = data_pseudo\n",
    "                    images_pseudo = to_var(images_pseudo)\n",
    "                    labels_pseudo = to_var(labels_pseudo)\n",
    "                    net_predictions_pseudo = net(images_pseudo)\n",
    "                    segmentation_classes_pseudo = getTargetSegmentation(labels_pseudo)\n",
    "                    loss_pseudo = loss(net_predictions_pseudo, segmentation_classes_pseudo)\n",
    "                except StopIteration:\n",
    "                    loss_pseudo = 0\n",
    "            else:\n",
    "                loss_pseudo = 0\n",
    "            \n",
    "            # Total loss\n",
    "            total_loss = 0\n",
    "            if loss_train == 0 and loss_pseudo == 0:\n",
    "                continue  # Skip if both losses are zero\n",
    "            elif loss_train == 0:\n",
    "                total_loss = loss_pseudo\n",
    "            elif loss_pseudo == 0:\n",
    "                total_loss = loss_train\n",
    "            else:\n",
    "                total_loss = (1 - gamma_loss) * loss_train + gamma_loss * loss_pseudo\n",
    "            \n",
    "            if loss_train != 0:\n",
    "                num_images_trained += images_train.size(0)\n",
    "            if loss_pseudo != 0:\n",
    "                num_images_trained += images_pseudo.size(0)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Append losses for monitoring\n",
    "            if loss_train != 0:\n",
    "                lossEpoch.append(loss_train.cpu().data.numpy())\n",
    "            if loss_pseudo != 0:\n",
    "                lossEpoch.append(loss_pseudo.cpu().data.numpy())\n",
    "\n",
    "        # Compute mean training loss for the epoch\n",
    "        lossEpoch = np.asarray(lossEpoch)\n",
    "        lossEpoch = lossEpoch.mean()\n",
    "        lossTotalTraining.append(lossEpoch)\n",
    "        print(f'Epoch {i} - Training Loss: {lossEpoch:.4f} - ', end='')\n",
    "        # Save the training losses\n",
    "        np.save(os.path.join(directory, 'Train_Losses.npy'), lossTotalTraining)\n",
    "\n",
    "\n",
    "        ########## VALIDATION PHASE ##########\n",
    "        # Set the model to evaluation mode\n",
    "        net.eval()\n",
    "        # Initialize metrics dictionary\n",
    "        val_metrics = {\n",
    "            'IoU': {cls: [] for cls in range(1, num_classes)},\n",
    "            'Precision': {cls: [] for cls in range(1, num_classes)},\n",
    "            'Recall': {cls: [] for cls in range(1, num_classes)},\n",
    "            'DSC': {cls: [] for cls in range(1, num_classes)},\n",
    "        }\n",
    "\n",
    "        ## BATCH VALIDATION LOOP\n",
    "        with torch.no_grad():  # No need to compute gradients during validation\n",
    "            for j, data in enumerate(val_loader):\n",
    "                if len(data[0]) == 0: continue # Skip empty batches\n",
    "\n",
    "                # Get the data\n",
    "                images, labels, img_names = data\n",
    "                # (images) Shape: (batch_size, 1, height, width) | torch.float32 | range: [0, 1]\n",
    "                # (labels) Shape: (batch_size, height, width) | torch.int64 | values: {0, 1, 2, 3}\n",
    "\n",
    "                # Move to GPU if available\n",
    "                labels = to_var(labels)\n",
    "                images = to_var(images)\n",
    "\n",
    "                # Forward pass\n",
    "                net_predictions = net(images) # Shape: (batch_size, num_classes, height, width) | torch.float32 | range: [0, 1]\n",
    "                # Get the target segmentation\n",
    "                segmentation_classes = getTargetSegmentation(labels) # Shape: (batch_size, height, width) | torch.int64 | values: {0, 1, 2, 3}\n",
    "\n",
    "                # Compute loss\n",
    "                loss_value = loss(net_predictions, segmentation_classes)\n",
    "                val_loss.append(loss_value.cpu().data.numpy())\n",
    "\n",
    "                # Get predicted segmentation masks\n",
    "                pred_masks = torch.argmax(net_predictions, dim=1) # Shape: (batch_size, height, width) | torch.int64 | values: {0, 1, 2, 3}\n",
    "\n",
    "                # Compute metrics\n",
    "                batch_metrics = compute_metrics(pred_masks, segmentation_classes, num_classes)\n",
    "                for metric_name in val_metrics:\n",
    "                    for cls in range(1, num_classes):\n",
    "                        val_metrics[metric_name][cls].extend(batch_metrics[metric_name][cls])\n",
    "                \n",
    "                # Print some of the validation images and predictions at the end of training\n",
    "                display_segmented_images(images, labels, pred_masks, num_images=images.size(1)) if (i == num_epochs - 1 and debug) else None\n",
    "\n",
    "        # Compute mean validation loss\n",
    "        val_loss_mean = np.mean(val_loss)\n",
    "        val_losses.append(val_loss_mean)  # Save the validation loss\n",
    "        print(f'Validation Loss: {val_loss_mean:.4f} - ', end='')\n",
    "        if pseudo_dataset is not None:\n",
    "            print(f'Training on {len(train_set_full)} labeled images and {len(pseudo_dataset)} unlabeled images')\n",
    "        else:\n",
    "            print(f'Training on {len(train_set_full)} labeled images')\n",
    "\n",
    "        # Save the metrics for plotting later\n",
    "        metrics_save_path = os.path.join(metrics_directory, f'metrics_epoch_{i}.npy')\n",
    "        np.save(metrics_save_path, val_metrics)\n",
    "\n",
    "        # Save the model if the validation loss is the best so far\n",
    "        if val_loss_mean < Best_loss_val:\n",
    "            Best_loss_val = val_loss_mean\n",
    "            BestEpoch = i\n",
    "            # Save the model\n",
    "            model_save_dir = os.path.join('./Models', model_name)\n",
    "            if not os.path.exists(model_save_dir):\n",
    "                os.makedirs(model_save_dir)\n",
    "            torch.save(net.state_dict(), os.path.join(model_save_dir, f'best_model_epoch_{i}.pth'))\n",
    "            print(f'Saved model at epoch {i} with validation loss {val_loss_mean}') if debug else None\n",
    "        \n",
    "\n",
    "        ########## SELF-TRAINING PHASE ##########\n",
    "        pseudo_labeled_data = None\n",
    "        if i == self_learning_epochs:\n",
    "            print(\"~~~~~~~~~~ Starting the self-training ~~~~~~~~~\")\n",
    "\n",
    "        if i >= self_learning_epochs and len(unlabeled_set) > 0:\n",
    "\n",
    "            net.eval()  # Set the model to evaluation mode\n",
    "            img_paths_to_remove = set() # Set to store image paths for removal from the unlabeled dataset\n",
    "            max_confidence = 0.0\n",
    "            \n",
    "            # Prediction loop for the unlabeled dataset\n",
    "            if unlabeled_loader is not None and len(unlabeled_set) > 0:\n",
    "                with torch.no_grad():\n",
    "                    for j, data in enumerate(unlabeled_loader):\n",
    "                        images, _, img_names = data  # (images) Shape: (batch_size, 1, height, width) | torch.float32 | range: [0, 1]\n",
    "                        images = to_var(images)\n",
    "\n",
    "                        # Forward pass\n",
    "                        net_predictions = net(images) # Shape: (batch_size, num_classes, height, width) | torch.float32\n",
    "\n",
    "                        # Apply softmax to get probabilities\n",
    "                        probs = softMax(net_predictions) # Shape: (batch_size, num_classes, height, width) | torch.float32 | range: [0, 1] (sums to 1 across classes)\n",
    "\n",
    "                        # Get predicted masks and max probabilities per pixel\n",
    "                        pred_masks = torch.argmax(probs, dim=1) # Shape: (batch_size, height, width) | torch.int64 | values: {0, 1, 2, 3}\n",
    "\n",
    "                        max_probs = torch.max(probs, dim=1)[0] # Shape: (batch_size, height, width) | torch.float32 | range: [0, 1] (max probability across classes)\n",
    "\n",
    "                        # Create a mask for foreground pixels (predicted as classes 1, 2, or 3)\n",
    "                        foreground_mask = (pred_masks >= 1) # Shape: (batch_size, height, width) | torch.bool (True where predicted class is >= 1)\n",
    "\n",
    "                        # Compute mean confidence over foreground pixels for each image\n",
    "                        mean_confidences = []\n",
    "                        for idx_in_batch in range(images.size(0)):\n",
    "                            # Extract foreground mask and max probabilities for the current image\n",
    "                            fg_mask = foreground_mask[idx_in_batch]  # Shape: (height, width) | torch.bool\n",
    "                            probs_fg = max_probs[idx_in_batch][fg_mask]  # Shape: (num_foreground_pixels) | torch.float32 | range: [0, 1]\n",
    "\n",
    "                            if probs_fg.numel() > 0:\n",
    "                                # Compute mean confidence over foreground pixels\n",
    "                                mean_confidence = probs_fg.mean()\n",
    "                            else:\n",
    "                                # If no foreground pixels are predicted, set mean confidence to zero\n",
    "                                mean_confidence = torch.tensor(0.0, device=probs.device)\n",
    "\n",
    "                            # Update the maximum confidence\n",
    "                            if mean_confidence.item() > max_confidence:\n",
    "                                max_confidence = mean_confidence.item()\n",
    "                            mean_confidences.append(mean_confidence.item())\n",
    "\n",
    "                        # For images where mean foreground confidence is above threshold\n",
    "                        for idx_in_batch, mean_confidence in enumerate(mean_confidences):\n",
    "                            if mean_confidence >= confidence_threshold:\n",
    "                                # Get original image and predicted mask\n",
    "                                original_image = images[idx_in_batch]\n",
    "                                pred_mask_original = pred_masks[idx_in_batch]\n",
    "\n",
    "                                # Apply random transformations and track them\n",
    "                                transformed_image, _, transformations_dict = augment(original_image)\n",
    "\n",
    "                                # Predict on the transformed image\n",
    "                                transformed_image = transformed_image.unsqueeze(0)\n",
    "                                net_predictions_transformed = net(transformed_image)\n",
    "                                probs_transformed = softMax(net_predictions_transformed)\n",
    "                                pred_mask_transformed = torch.argmax(probs_transformed, dim=1)\n",
    "\n",
    "                                # De-transform the transformed mask\n",
    "                                pred_mask_de_transformed = de_transform_mask(pred_mask_transformed.squeeze(0), transformations_dict)\n",
    "\n",
    "                                # Compute similarity between the two masks\n",
    "                                mean_iou = compute_mean_iou(\n",
    "                                    pred_mask_original.cpu(), pred_mask_de_transformed.cpu(), num_classes\n",
    "                                )\n",
    "\n",
    "                                # Some debugging\n",
    "                                print(f\"Mean IoU: {mean_iou} - Confidence: {mean_confidence}\") if debug else None\n",
    "\n",
    "                                # If the IoU is above a threshold, add the pseudo-labeled data\n",
    "                                if mean_iou >= similarity_threshold:\n",
    "                                    pseudo_image = images[idx_in_batch].cpu() # Shape: (1, height, width) | torch.float32 | range: [0, 1]\n",
    "                                    pseudo_mask_indices = pred_masks[idx_in_batch].cpu() # Shape: (height, width) | torch.int64 | values: {0, 1, 2, 3}\n",
    "                                    value_map = torch.tensor([0.0, 0.33333334, 0.6666667, 0.94117647]) # Mapping of class indices to values\n",
    "                                    pseudo_mask = value_map[pseudo_mask_indices] # Shape: (height, width) | torch.float32 | range: [0, 1]\n",
    "                                    \n",
    "                                    pseudo_img_name = img_names[idx_in_batch]\n",
    "\n",
    "                                    # Add pseudo-labeled data to the list\n",
    "                                    if pseudo_labeled_data is None:\n",
    "                                        pseudo_labeled_data = [(pseudo_image, pseudo_mask, pseudo_img_name)]\n",
    "                                    else:\n",
    "                                        pseudo_labeled_data.append((pseudo_image, pseudo_mask, pseudo_img_name))\n",
    "\n",
    "                                    # Mark image path for removal\n",
    "                                    img_paths_to_remove.add(pseudo_img_name)\n",
    "            \n",
    "            \n",
    "            # Add good pseudo-labeled data to the training set\n",
    "            if pseudo_labeled_data is not None:\n",
    "                print(f\"Number of pseudo-labeled images added: {len(pseudo_labeled_data)}\") if debug else None\n",
    "\n",
    "                # Create a dataset for the pseudo-labeled data\n",
    "                # (item) Shape: (image, mask, img_name)\n",
    "                #     - (image) Shape: (1, height, width) | torch.float32 | range: [0, 1]\n",
    "                #     - (mask) Shape: (height, width) | torch.float32 | range: [0, 1]\n",
    "                #     - (img_name) String\n",
    "                new_pseudo_dataset = medicalDataLoader.MedicalImageDataset(\n",
    "                    mode='pseudo',\n",
    "                    root_dir=root_dir,\n",
    "                    transform=transform,\n",
    "                    mask_transform=mask_transform,\n",
    "                    augment=True,\n",
    "                    equalize=True,\n",
    "                    data_list=pseudo_labeled_data\n",
    "                )\n",
    "                # Visualize the pseudo-labeled samples\n",
    "                display_dataset_samples(new_pseudo_dataset, num_samples=1) if debug else None\n",
    "\n",
    "                # If it's the first time, assign pseudo_dataset\n",
    "                if pseudo_dataset is None:\n",
    "                    pseudo_dataset = new_pseudo_dataset\n",
    "                else:\n",
    "                    # Update the pseudo_dataset with new data\n",
    "                    pseudo_dataset = ConcatDataset([pseudo_dataset, new_pseudo_dataset])\n",
    "\n",
    "                # Create a DataLoader for the pseudo-labeled data\n",
    "                pseudo_loader = DataLoader(\n",
    "                    pseudo_dataset,\n",
    "                    batch_size=batch_size,\n",
    "                    worker_init_fn=np.random.seed(0),\n",
    "                    num_workers=0,\n",
    "                    shuffle=True,\n",
    "                    drop_last=True\n",
    "                )\n",
    "\n",
    "                # Remove pseudo-labeled images from the unlabeled dataset\n",
    "                unlabeled_set.remove_items(img_paths_to_remove)\n",
    "                unlabeled_loader = DataLoader(\n",
    "                    unlabeled_set,\n",
    "                    batch_size=batch_size,\n",
    "                    worker_init_fn=np.random.seed(0),\n",
    "                    num_workers=0,\n",
    "                    shuffle=False\n",
    "                )\n",
    "\n",
    "                # Clear the pseudo-labeled data for the next iteration\n",
    "                pseudo_labeled_data = None\n",
    "            else:\n",
    "                print(\"No pseudo-labeled images were confident enough to be added.\") if debug else None\n",
    "        \n",
    "\n",
    "        # Print number of images trained\n",
    "        print(f'Images trained on: {num_images_trained}')\n",
    "    \n",
    "    print(f\"Training completed. Best model saved at epoch {BestEpoch} with validation loss {Best_loss_val}\")\n",
    "\n",
    "    # Save the validation losses\n",
    "    np.save(os.path.join(directory, 'Val_Losses.npy'), val_losses)\n",
    "\n",
    "    # Plot the metrics\n",
    "    plot_metrics(model_name, num_epochs, num_classes)\n",
    "    # Plot the losses\n",
    "    plot_losses(model_name)\n",
    "    \n",
    "    # Return best loss\n",
    "    return Best_loss_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45519f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_TEST = 24\n",
    "\n",
    "def test_model(model_name, batch_size_test=BATCH_SIZE_TEST, dataset='val'):\n",
    "    # Paths\n",
    "    model_dir = os.path.join('./models', model_name)\n",
    "    results_dir = os.path.join('./Results/Images', model_name)\n",
    "    if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "\n",
    "    # Load the best model (highest epoch number)\n",
    "    model_files = [f for f in os.listdir(model_dir) if f.endswith('.pth')]\n",
    "    if not model_files:\n",
    "        print(f\"No model files found in {model_dir}\")\n",
    "    else:\n",
    "        # Extract epoch numbers (\"best_model_epoch_XXX.pth\")\n",
    "        epoch_numbers = [int(f.split('_')[-1].split('.')[0]) for f in model_files]\n",
    "        # Find the model with the highest epoch number\n",
    "        best_epoch = max(epoch_numbers)\n",
    "        best_model_file = f'best_model_epoch_{best_epoch}.pth'\n",
    "        best_model_path = os.path.join(model_dir, best_model_file)\n",
    "        print(f\"Loading model: {best_model_path}\")\n",
    "\n",
    "        # Initialize the model\n",
    "        net = UNet(4)\n",
    "        net.load_state_dict(torch.load(best_model_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu')))\n",
    "        if torch.cuda.is_available():\n",
    "            net.cuda()\n",
    "        net.eval()\n",
    "\n",
    "        # Prepare the test dataset (subset of validation set)\n",
    "        root_dir = './Data/'\n",
    "\n",
    "        # Data transforms\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        mask_transform = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "        # Load the test dataset\n",
    "        test_set = medicalDataLoader.MedicalImageDataset(dataset,\n",
    "                                                         root_dir,\n",
    "                                                         transform=transform,\n",
    "                                                         mask_transform=mask_transform,\n",
    "                                                         equalize=True)\n",
    "        test_loader = DataLoader(test_set,\n",
    "                                batch_size=batch_size_test,\n",
    "                                worker_init_fn=np.random.seed(0),\n",
    "                                num_workers=0,\n",
    "                                shuffle=False)\n",
    "        \n",
    "        # Compute class weights for the cross-entropy loss\n",
    "        initial_pixel_counts = compute_class_pixel_counts(test_set, 4)\n",
    "        class_weights = compute_class_weights(initial_pixel_counts)\n",
    "\n",
    "        # Run inference\n",
    "        mean_loss, mean_dice = inference(net, test_loader, model_name, class_weights)\n",
    "\n",
    "        print(f\"Mean Cross-Entropy Loss on test set: {mean_loss}\")\n",
    "        print(f\"Mean Dice Coefficient on test set: {mean_dice}\\n\")\n",
    "\n",
    "        return mean_loss, mean_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b0be014eac86ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T20:59:50.149405Z",
     "start_time": "2024-11-21T20:58:43.287225Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Train a model\n",
    "#runTraining()\n",
    "\n",
    "# Test a model\n",
    "#test_model(\"0_Base_Model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
